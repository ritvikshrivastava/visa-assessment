# Documentation

### Design
* The API ``visa_assessment/api.py`` receives as input a CV file and assesses the text within the file against
the eligibility criteria present in ``visa_assessment/eligibility.py`` using OpenAI's GPT-4o model.

* Zero-Shot Model Prompting: The model is prompted to check against all levels of evidence in our static eligibility document.

* Information Retrieval Step: Using the eligibility document as a resource, the model parses the text of the resume 
and finds corresponding matches to each criterion/eligibility category.  This code can be found in `visa_assessment/judge.py``

* Once this list is collected, a rating is generated based on the assumptions described later.

* Testing and validation of the responses was performed, and details are at the end of this document.
* Sample code to run the example is in the README.


### API response
API response is a JSON response with 
* "eligibility criterion" as the key corresponding the the criteria matching the requirements from the CV
* "rating": chances of eligibility

### Assumptions
* CV files can be of two types of inputs: PDFs, or text files 
* Categorization: High means atleast 5 criteria being met by evidence in CV, 3-4 means Medium, and the rest give a rating of low.
* Modeling and Information Retrieval: 
  * For the sake of experimentation using APIs and to cover for the lack of local GPU access to efficiently run open-source models, this project uses GPT-4o as the model of choice to leverage the LLM capabilities. I implemented this using Azure Open AI, but added additional code for it to be run using OpenAI directly. 
  * This can be replaced by any other proprietary model, say Claude/Gemini or any open source model say Llama3 or Mistral7b-instruct if resources are available.
  * A temperature of 0 was chosen as a strict measure to prevent hallucination or creativity and sticking to responses within the text.
  * The model token limit for responses was set to 1024.
* Criteria has been collected from the USCIS sources.
  * This data was then summarized in one-pass using a zero-shot GPT-4o run. This criteria was then stored as a dynamic evidence checking resource in ``visa_assessments/eligiblity.py``
  * This static data source is broken into two pieces per category of eligibility: description, and evidence types.

### Testing
* The ``tests/`` directory covers unittests to check for correctness
* The system has ``black`` and ``isort`` installed with poetry to clean-up the code.
* There is a test file included with Albert Einstein's synthetically generated resume in ``test_cv.txt``
* Following instructions to run the code in ``README.md``, the following response is generated by the API:

```json
{
  "eligibility criterion": {
    "Awards": [
      "1921 Nobel Prize in Physics for the explanation of the photoelectric effect"
    ],
    "Membership": [
      "Membership in prestigious scientific societies (details can be added if known)"
    ],
    "Press": [
      "Featured in prominent media for his groundbreaking work (Provide specific examples if possible)"
    ],
    "Judging": [],
    "Original contribution": [
      "Special Relativity (1905): Published in 'Annalen der Physik'",
      "General Relativity (1915): Published in 'Sitzungsberichte der Preußischen Akademie der Wissenschaften'",
      "Photoelectric Effect Explanation (1905): Published in 'Annalen der Physik'",
      "Contributions to Quantum Mechanics: Published in various scientific journals"
    ],
    "Scholarly articles": [
      "Special Relativity (1905): Published in 'Annalen der Physik'",
      "General Relativity (1915): Published in 'Sitzungsberichte der Preußischen Akademie der Wissenschaften'",
      "Photoelectric Effect Explanation (1905): Published in 'Annalen der Physik'",
      "Contributions to Quantum Mechanics: Published in various scientific journals"
    ],
    "Critical employment": [
      "Professor of Theoretical Physics at Institute for Advanced Study, Princeton, NJ, USA",
      "Professorships at Charles-Ferdinand University (Prague), ETH Zurich, Kaiser Wilhelm Institute for Physics (Berlin)"
    ],
    "High remuneration": [
      "Salaries commensurate with his prominent positions (details can be added if known)"
    ]
  },
  "rating": "high"
}
```

### Evaluation (Implemented Measures)
#### 1. Gold-set evaluation
   * Synthetically created a gold set of 20 resumes (``gold_set.json``) using a different LLM, with different labels of low-medium-high eligibility. 
      Checked manually for annotation.
   * Evaluation script: ``evaluation/gold_eval.py``
   * Ran a win-loss analysis for the predicted set against the gold set
   * This evaluation is a way to ensure the overall results of the system.

#### 2. Natural Language Inference (NLI) Scorer
  * To check the accuracy of each of the retrieved components, evaluate LLM Responses using _Retrieval_ against the original document (CV):
  * Step 1: Chunk the original CV document into chunks of text
  * Step 2: Find the closest matching chunk for each criterion of information by querying it against the CV using a retrieval model.
  * For the sake of simplicity, the LLM of choice (GPT-4o) serves as the retriever module. Models like Approximate Nearest Neighbors/BM-25 or  ColBERT can be chosen in a more expansive system. 
  * Step 3: Run a NLI/MNLI scorer (e.g. BART-large-nli) model to calculate NLI score. If the results show entailment, score is marked as correct, otherwise negative in the case of contradiction.
  * Judgements can be further made on human evaluation to assess the scores of the MNLI model and what threshold of acceptance needs to be set for entailments.
   

### Additional suggested evaluation measures (not-implemented)
#### Correctness - Check for consistency and correctness using QAEval. 
  * Generate questions against the source document (CV) and (gold-label) answers against the same source document.
  * Test for answers generated for the questions when queried against the response document ("eligibility criterion" in the results.)
